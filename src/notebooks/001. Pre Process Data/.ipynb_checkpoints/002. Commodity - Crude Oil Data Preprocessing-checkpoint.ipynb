{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Challenge\n",
    "\n",
    "Commodity stock corresponding to a particular month expires at a particular date on the same month or the very next month For example, Crude Oil Stock for a particular month expires on 18th of the next calendar month or the last working day before the 18th of the next calendar day if it falls on a stock market holiday\n",
    "\n",
    "Hence a CrudeOil Stock of March shall expire on 18th of April for a given year or on a working day prior to 18th if 18th is a holiay\n",
    "\n",
    "Typically one can buy a particular month's stock way a head of time. Say you can buy a Crude oil stock corresponding to the month of june on Jan which is 6 months a head\n",
    "\n",
    "So, accumulating data a head of time for machine learning/analysis is pretty complex and confusing.\n",
    "\n",
    "I have deviced the following way to over come this complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "## 1. Download Source data - Manual Process\n",
    "- Download data from decided known source and save as such as json file in the '**dataName**/data/raw/**frequency**' folder\n",
    "- every time you save a new fragment of data, make sure that you are saving the file with names which are always in ascending order alphabetically. For example, save new files with the name confining to the series say '01.json,02.json,03.json ... and so on..' or like 'aa.json, ab.json, aca.json,acab.json,ad.json and so on...' \n",
    "- Please try to follow an ascending alphabetic file pattern for every for every new file name to AVOID issues.\n",
    "- Should still work with casual file names as well\n",
    "- Download and save data this way as frequent as possible for as many products as required\n",
    "\n",
    "## 2. Configure Input Cell - Manual Process\n",
    "- Provide the following inputs namely\n",
    "    - dataName\n",
    "    - dataFrequency\n",
    "    - stockExpiresOnDate\n",
    "    \n",
    "For crudeOil a typical example would be\n",
    "\n",
    "> dataName = 'crudeOil_India' <br>\n",
    "  dataFrequency = '5m'<br>\n",
    "  stockExpiresOnDate = 18\n",
    "\n",
    "Do not modify any other code present in the input cell and change only the other input parameter named 'noteBookRelativeDepthFromRoot' only if necessary\n",
    "\n",
    "This will take care of the initializing the Jupyter Notebook environment to perform the desired data manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create  !!dataName!!_HistoricalData.csv - Manual ( Semi Automatic) \n",
    "> (eg crudeOil_India_HistoricalData.csv)<br>\n",
    "(One time only) <br>\n",
    "**NOTE**: You may **NOT have to repeat this step** if it has been done in the past\n",
    "\n",
    "- a file with frozen historical market data for each commodity has to be saved in the following folder location:\n",
    "    - '**!!dataName!!**/data/raw/**!!dataFrequency!!**/frozenHistoricalData' \n",
    "- gather as much data as possible for the current running month's stock\n",
    "- define the data as \"**dataName**H\n",
    "\", for example \"crudeOilHistoricalData.csv\". Freeze this data.\n",
    "- data can only be added to this file but never any entry should be modified\n",
    "\n",
    "## 4. Create !!dataName!!_!!month!!~.csv - Manual ( Semi Automatic)\n",
    "> (eg crudeOil_Jun.csv) \n",
    "\n",
    "- gather current month data till 18th of the next calendar month as much as possible\n",
    "- remove all entries in the current  month data for which historical data is already present in say 'crudeOilHistoricalData.csv)\n",
    "- add the remaining data in the current month to the historical data and call it as say 'crudeOil_mar.csv'\n",
    "\n",
    "## 5. Update and Archive Expiried Commodity Stock Data - Manual ( Semi Automatic)\n",
    "- as soon as stock for the current month expires, update the historical data with the existing data and archive the current month based file\n",
    "\n",
    "## Update crudeOilHistoricalData.csv every cycle and repeat previous step\n",
    "\n",
    "- When a new month's stock starts every 19th of the month archive the 'crudeOilHistoricalData.csv' file\n",
    "- Rename 'crudeOil_month.csv' into 'crudeOilHistoricalData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_path >>> /Users/493055/Desktop/dev/erai-v0.3/src <<< appended to sys path - Mandatory for the pythons files in src folder to be recognized\n"
     ]
    }
   ],
   "source": [
    "dataName = 'crudeOil_India'\n",
    "dataFrequency='5m'\n",
    "stockExpiresOnDate = 18\n",
    "\n",
    "# DO NOT MODIFY THE FOLLOWING UNLESS THE NOTEBOOK'S LOCATION HAS CHANGED AND IS NECESSARY\n",
    "noteBookRelativeDepthFromRoot = '../../'\n",
    "\n",
    "import os\n",
    "import sys  \n",
    "module_path = os.path.abspath(noteBookRelativeDepthFromRoot + os.path.join('.'))\n",
    "\n",
    "sys.path.append(module_path)\n",
    "\n",
    "print(\"module_path >>> \" + module_path + \" <<< appended to sys path - Mandatory for the pythons files in src folder to be recognized\")\n",
    "\n",
    "\n",
    "# from dataPreprocessing import *\n",
    "# from dataPreprocessing.preProcessData import collateData\n",
    "# df = collateData(dataName,dataFrequency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preprocessing raw commodity data\n",
      "looping through all the files to create input data\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/01.json ...\n",
      "File read - SUCCESS\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/06.json ...\n",
      "File read - SUCCESS\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/07.json ...\n",
      "File read - SUCCESS\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/04.json ...\n",
      "File read - SUCCESS\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/08.json ...\n",
      "File read - SUCCESS\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/09.json ...\n",
      "File read - SUCCESS\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/05.json ...\n",
      "File read - SUCCESS\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/02.json ...\n",
      "File read - SUCCESS\n",
      "reading input file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/03.json ...\n",
      "File read - SUCCESS\n",
      " checking if folder existis >>>/Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/frozenHistoricalData\n",
      "Debug >>> start >>> isFileExists method\n",
      "Debug >>> isFileExists >>> projectRootFolderPath = /Users/493055/Desktop/dev/erai-v0.3\n",
      "Debug >>> isFileExists >>> valueError\n",
      "DEBUG >>> isFileExists >>> rootFolderPathIndex = -1\n",
      "Debug >>> check if file path exists >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/frozenHistoricalData/crudeOil_India_HistoricalData.csv\n",
      "Debug >>> end >>> isFileExists method\n",
      "historical record not present - created one with existing data\n",
      " debug >>> archiveFileInRawDataFolder >>> into method\n",
      " checking if folder existis >>>/Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive\n",
      " checking if folder existis >>>/Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788\n",
      "/Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788 >>> Folder created\n",
      " debug >>> archiveFileInRawDataFolder >>> about to loop through filelist\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/01.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/01.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/01.json\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/06.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/06.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/06.json\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/07.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/07.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/07.json\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/04.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/04.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/04.json\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/08.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/08.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/08.json\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/09.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/09.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/09.json\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/05.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/05.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/05.json\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/02.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/02.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/02.json\n",
      " debug >>> archiveFileInRawDataFolder >>> attempting to copy file >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/03.json\n",
      "to location >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/archive/archive_1583236675.08788/03.json\n",
      "File deleted >>> /Users/493055/Desktop/dev/erai-v0.3/data/crudeOil_India/raw/5m/03.json\n",
      " debug >>> archiveFileInRawDataFolder >>> completed method\n",
      " debug >>> archiveFileInRawDataFolder >>> returning value >>> True\n",
      "debug >>> all files archived successfully\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-68bcd676fab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcummulativeRawDataDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputFilePath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreProcessCommodityData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataFrequency\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstockExpiresOnDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-68bcd676fab0>\u001b[0m in \u001b[0;36mpreProcessCommodityData\u001b[0;34m(dataName, dataFrequency, stockExpiresOnDate)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcummulativeRawDataDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputFilePath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreProcessCommodityData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataFrequency\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstockExpiresOnDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reset_index'"
     ]
    }
   ],
   "source": [
    "traceback_template = '''Traceback (most recent call last):\n",
    "  File \"%(filename)s\", line %(lineno)s, in %(name)s\n",
    "%(type)s: %(message)s\\n'''  # Skipping the \"actual line\" item\n",
    "\n",
    "def preProcessCommodityData(dataName,dataFrequency,stockExpiresOnDate):\n",
    "    import os\n",
    "    import sys\n",
    "    import traceback\n",
    "\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "\n",
    "    from utils.errorHandler import handleError\n",
    "    from utils.common import getProjectRootFolderPath\n",
    "\n",
    "    from utils.fileFolderManipulations import getParentFolder\n",
    "    from utils.fileFolderManipulations import createFolder\n",
    "    from utils.fileFolderManipulations import isFileExists\n",
    "    from utils.fileFolderManipulations import archiveFileInRawDataFolder\n",
    "\n",
    "    from config.config import getAppConfigData\n",
    "    from config.config import setAppConfigData\n",
    "    \n",
    "    # return values of this method\n",
    "    # -------------------------------------------------------------------------------\n",
    "    # complete filepath of the csv file with the processed raw data\n",
    "    outputFilePath = None\n",
    "    # Variable to hold a dataframe created with the data from input data files in the relativeDataFolderPath provided\n",
    "    cummulativeRawDataDf = None\n",
    "        \n",
    "    try:\n",
    "        print(' Preprocessing raw commodity data')\n",
    "        \n",
    "        # declaring variables and objects necessary for further manipulations\n",
    "        # holds the relative data folder path corresponding to the current data and dataFrequency\n",
    "        relativeDataFolderPath = 'data/'+dataName+'/raw/' + dataFrequency\n",
    "        \n",
    "        # Variable to hold the original source folder path which is calculated from the input relative path of the source folder (relativeDataFolderPath)\n",
    "        # using various python commands like os.path.abspath and os.path.join\n",
    "        projectRootFolderPath = None\n",
    "\n",
    "        # Variable to hold the original source folder path which is calculated from the input relative path of the source folder (relativeDataFolderPath)\n",
    "        # using various python commands like os.path.abspath and os.path.join\n",
    "        dataFolderPath = None\n",
    "\n",
    "        # Variable to hold query like value of python to query all json file names in the source folder (dataFolderPath).\n",
    "        # Will be used in the glob function to execute the query\n",
    "        json_pattern = None\n",
    "\n",
    "        # Variable to contain the list of all input json file names in the source folder (dataFolderPath)\n",
    "        file_list = None\n",
    "\n",
    "        # Current methods return value initialized to false. Will be maked as true\n",
    "        # after every single line in the method has been executed with out errors\n",
    "        returnValue = False\n",
    "        \n",
    "        # complete filepath of the csv file with the processed raw data\n",
    "        outputFolderName = None\n",
    "        inputRawDataDF = None\n",
    "        \n",
    "        # Algorithm :: Source Code\n",
    "        \n",
    "        # caluclate the deployment directory path of the current juypter node in the operating system\n",
    "        projectRootFolderPath = getProjectRootFolderPath()\n",
    "\n",
    "        # TO BE MODIFIED - NOT SURE WHY I USED THIS - WILL HAVE TO CHECK\n",
    "        pd.set_option('display.max_columns', None)\n",
    "\n",
    "        # creating pandas dataframe references for further modification\n",
    "        inputRawDataDF = pd.DataFrame()\n",
    "\n",
    "        # calculating the complete data folder path of the relative path provided as parameter\n",
    "        dataFolderPath = projectRootFolderPath + '/'+relativeDataFolderPath\n",
    "\n",
    "        # creating OS queryable object for python to work with to find json files in the dataFolderPath calcuated in the previous step\n",
    "        json_pattern = os.path.join(dataFolderPath, '*.json')\n",
    "\n",
    "        # store all the json file paths in the dataFolderPath for further processing\n",
    "        file_list = glob.glob(json_pattern)\n",
    "        \n",
    "        # execution assertion/ui progress update info\n",
    "        print('looping through all the files to create input data')\n",
    "        # loop through all the files in the folder and create inputRawDataDF pandas datafram\n",
    "        for file in file_list:\n",
    "            print(\"reading input file >>> \" + file + \" ...\")\n",
    "            data=None\n",
    "            try:\n",
    "                # read json data from unformatted json file\n",
    "                data = pd.read_json(file, lines=True)\n",
    "                \n",
    "                if isinstance(data, str):\n",
    "                    data = data['data'][0]['candles']\n",
    "                elif isinstance(data,pd.DataFrame):\n",
    "                    data = data['data'][0]['candles']\n",
    "                else:\n",
    "                    data = data.values[0][0]['candles']\n",
    "            \n",
    "            except ValueError:\n",
    "                # read data from formatted / json linted json file\n",
    "                data = pd.read_json(file)\n",
    "\n",
    "                data = data['data'][0]\n",
    "            \n",
    "\n",
    "            inputRawDataDF = inputRawDataDF.append(data, ignore_index=True)\n",
    "            print(\"File read - SUCCESS\")\n",
    "\n",
    "        #assign column names \n",
    "        inputRawDataDF.columns = ['date-time', 'open',\n",
    "                                  'high', 'low', 'close', 'quantity', 'dont-know']\n",
    "        inputRawDataDF_native = inputRawDataDF\n",
    "        \n",
    "        inputRawDataDF_native['date'] = pd.to_datetime(inputRawDataDF_native['date-time']) \n",
    "        \n",
    "        inputRawDataDF_native=inputRawDataDF_native.sort_values('date-time',ascending=True)\n",
    "        inputRawDataDF_native=inputRawDataDF_native.set_index(keys=['date-time'])\n",
    "        \n",
    "        #drop duplicate records\n",
    "        inputRawDataDF_native=inputRawDataDF_native.groupby(inputRawDataDF_native.index).last()\n",
    "        #inputRawDataDF_native=inputRawDataDF_native.drop_duplicates(keep='last')\n",
    "        \n",
    "        historicalFrozenDataFolderPath = relativeDataFolderPath + '/frozenHistoricalData'\n",
    "        \n",
    "        # create folder named 'frozenHistoricalData' if it does not exist\n",
    "        createFolder(historicalFrozenDataFolderPath)\n",
    "        \n",
    "        historicalRecordFilePath = historicalFrozenDataFolderPath + '/' + dataName + \"_HistoricalData.csv\"\n",
    "        \n",
    "        # if historical records exist then \n",
    "        # 1. get all historical values \n",
    "        # 2. get all current raw data values provided\n",
    "        # 3. check if there is an updated value for the last value present in the historical records in the current \n",
    "        #     record, if yes use it, if not use the last value in the historical record as such\n",
    "        # 4. get all values after the last date-time entry in the current record\n",
    "        # 5. add historicalData(including last value)+ currentData(after penultimate value in historical records)\n",
    "        # 6. there is a possible duplication of last value in historical data, due to an updated value in current values\n",
    "        #      so, drop duplicates and keep last updated value\n",
    "        \n",
    "        historicalRecordExists = False\n",
    "        if isFileExists(historicalRecordFilePath):\n",
    "            print ('historical record exists')\n",
    "            historicalRecordExists = True\n",
    "            \n",
    "            # 1. get all historical data\n",
    "            historicalRecordsDf = pd.read_csv(projectRootFolderPath + '/' +historicalRecordFilePath)\n",
    "            \n",
    "            # 2. get all current raw data values provided\n",
    "            inputRawDataDF_native\n",
    "            \n",
    "            # 3. get all values after the last date-time entry in the current record\n",
    "            cutoffDateFromPenultimateValueInHistoricalData = historicalRecordsDf[historicalRecordsDf.shape[0]-2:]['date-time'].values[0]\n",
    "            print('cutoffDateFromPenultimateValueInHistoricalData = '+cutoffDateFromPenultimateValueInHistoricalData)\n",
    " \n",
    "            mask = (inputRawDataDF_native['date'] > cutoffDateFromPenultimateValueInHistoricalData) # & (df['date'] <= end_date)\n",
    "            newValuesDf = inputRawDataDF_native.loc[mask]\n",
    "            \n",
    "            # 4. check if there is an updated value for the last value present in the historical records in the current \n",
    "            #     record, if yes use it, if not use the last value in the historical record as such\n",
    "            # greater than the start date and smaller than the end date\n",
    "            # This can be perfromed  by the following formula\n",
    "            #  add historicalData(including last value)+ currentData(after penultimate value in historical records)\n",
    "            \n",
    "            newValuesDf = newValuesDf.drop(['date'],axis=1)\n",
    "            newValuesDf = newValuesDf.reset_index()\n",
    "            print('Debug >>> newValuesDf droped date column')\n",
    "            \n",
    "            cummulativeRawDataDf = pd.concat([historicalRecordsDf,newValuesDf],axis=0,sort=True)\n",
    "            \n",
    "            # 5. there is a possible duplication of last value in historical data, due to an updated value in current values\n",
    "            #      so, drop duplicates and keep last updated value\n",
    "            cummulativeRawDataDf = cummulativeRawDataDf.sort_values('date-time',ascending=True)\n",
    "            cummulativeRawDataDf = cummulativeRawDataDf.set_index(keys=['date-time'])\n",
    "\n",
    "            #drop duplicate records\n",
    "            cummulativeRawDataDf = cummulativeRawDataDf.groupby(cummulativeRawDataDf.index).last() \n",
    "            \n",
    "            \n",
    "        if historicalRecordExists:\n",
    "            cummulativeRawDataDf.to_csv(projectRootFolderPath + '/' + historicalRecordFilePath, sep=',', index=True)\n",
    "            cummulativeRawDataDf = cummulativeRawDataDf.reset_index()\n",
    "            print ('historical record data updated')\n",
    "        else:\n",
    "            #inputRawDataDF_native['date-time']=inputRawDataDF_native.index\n",
    "            inputRawDataDF_native.to_csv(projectRootFolderPath + '/' + historicalRecordFilePath, sep=',', index=True)\n",
    "            cummulativeRawDataDf = inputRawDataDF_native\n",
    "            cummulativeRawDataDf = cummulativeRawDataDf.reset_index()\n",
    "            print ('historical record not present - created one with existing data')\n",
    "        \n",
    "        #archive raw data files\n",
    "        archiveFileInRawDataFolder(dataName,dataFrequency,file_list)\n",
    "            \n",
    "        print('debug >>> all files archived successfully')\n",
    "    except:\n",
    "        handleError(sys.exc_info(), traceback, traceback_template)\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        return cummulativeRawDataDf.reset_index(),outputFilePath\n",
    "    \n",
    "x = preProcessCommodityData(dataName,dataFrequency,stockExpiresOnDate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=x[0]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # caluclate the deployment directory path of the current juypter node in the operating system\n",
    "# projectRootFolderPath = getProjectRootFolderPath()\n",
    "\n",
    "# # TO BE MODIFIED - NOT SURE WHY I USED THIS - WILL HAVE TO CHECK\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # creating pandas dataframe references for further modification\n",
    "# inputRawDataDF = pd.DataFrame()\n",
    "\n",
    "# # calculating the complete data folder path of the relative path provided as parameter\n",
    "# dataFolderPath = projectRootFolderPath + '/'+relativeDataFolderPath\n",
    "\n",
    "# # creating OS queryable object for python to work with to find json files in the dataFolderPath calcuated in the previous step\n",
    "# json_pattern = os.path.join(dataFolderPath, '*.json')\n",
    "\n",
    "# # store all the json file paths in the dataFolderPath for further processing\n",
    "# file_list = glob.glob(json_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list.sort()\n",
    "# file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for file in file_list:\n",
    "#     print(\"reading input file >>> \" + file + \" ...\")\n",
    "#     data=None\n",
    "#     try:\n",
    "#         # read json data from unformatted json file\n",
    "#         data = pd.read_json(file, lines=True)\n",
    "\n",
    "#         if isinstance(data, str):\n",
    "#             data = data['data'][0]['candles']\n",
    "#         elif isinstance(data,pd.DataFrame):\n",
    "#             data = data['data'][0]['candles']\n",
    "#         else:\n",
    "#             data = data.values[0][0]['candles']\n",
    "\n",
    "#     except ValueError:\n",
    "#         # read data from formatted / json linted json file\n",
    "#         data = pd.read_json(file)\n",
    "\n",
    "#         data = data['data'][0]\n",
    "\n",
    "\n",
    "#     inputRawDataDF = inputRawDataDF.append(data, ignore_index=True)\n",
    "#     print(\"File read - SUCCESS\")\n",
    "\n",
    "# #assign column names\n",
    "# inputRawDataDF.columns = ['date-time', 'open',\n",
    "#                           'high', 'low', 'close', 'quantity', 'dont-know']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawdataDf  = inputRawDataDF \n",
    "# rawdataDf.index = rawdataDf['date-time']\n",
    "# rawdataDf = rawdataDf.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rawdataDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rawdataDf=rawdataDf.groupby(inputRawDataDF.index).last()\n",
    "# rawdataDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawdataDf.to_csv(projectRootFolderPath+'/01.crudeoil_feb_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataName = 'crudeoil_india'\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import traceback\n",
    "\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# from utils.errorHandler import handleError\n",
    "# from utils.common import getProjectRootFolderPath\n",
    "\n",
    "# from utils.fileFolderManipulations import getParentFolder\n",
    "# from utils.fileFolderManipulations import createFolder\n",
    "\n",
    "# from config.config import getAppConfigData\n",
    "# from config.config import setAppConfigData\n",
    "\n",
    "# from fastai.tabular import add_datepart\n",
    "\n",
    "# from dataPreparation.simpleFeatures import getFollowingHolidaysDaysStamp\n",
    "# from dataPreparation.simpleFeatures import getPriorHoliDaysStamps\n",
    "\n",
    "# print(' data pre-processing >> imported dependencies')\n",
    "\n",
    "# relativeDataFolderPath = 'data/'+dataName+'/raw/' + dataFrequency\n",
    "\n",
    "# # Variable to hold the original source folder path which is calculated from the input relative path of the source folder (relativeDataFolderPath)\n",
    "# # using various python commands like os.path.abspath and os.path.join\n",
    "# projectRootFolderPath = None\n",
    "\n",
    "# # Variable to hold a dataframe created with the data from input data files in the relativeDataFolderPath provided\n",
    "# inputRawDataDF = None\n",
    "\n",
    "# # Variable to hold the original source folder path which is calculated from the input relative path of the source folder (relativeDataFolderPath)\n",
    "# # using various python commands like os.path.abspath and os.path.join\n",
    "# dataFolderPath = None\n",
    "\n",
    "# # Variable to hold query like value of python to query all json file names in the source folder (dataFolderPath).\n",
    "# # Will be used in the glob function to execute the query\n",
    "# json_pattern = None\n",
    "\n",
    "# # Variable to contain the list of all input json file names in the source folder (dataFolderPath)\n",
    "# file_list = None\n",
    "\n",
    "# # return values of this method\n",
    "# # -------------------------------------------------------------------------------\n",
    "# # Current methods return value initialized to false. Will be maked as true\n",
    "# # after every single line in the method has been executed with out errors\n",
    "# returnValue = False\n",
    "# # complete filepath of the csv file with the processed raw data\n",
    "# outputFilePath = None\n",
    "# outputFolderName = None\n",
    "\n",
    "# # caluclate the deployment directory path of the current juypter node in the operating system\n",
    "# projectRootFolderPath = getProjectRootFolderPath()\n",
    "\n",
    "# # TO BE MODIFIED - NOT SURE WHY I USED THIS - WILL HAVE TO CHECK\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # creating pandas dataframe references for further modification\n",
    "# inputRawDataDF = pd.DataFrame()\n",
    "\n",
    "# # calculating the complete data folder path of the relative path provided as parameter\n",
    "# dataFolderPath = projectRootFolderPath + '/'+relativeDataFolderPath\n",
    "\n",
    "# # creating OS queryable object for python to work with to find json files in the dataFolderPath calcuated in the previous step\n",
    "# json_pattern = os.path.join(dataFolderPath, '*.json')\n",
    "\n",
    "# # store all the json file paths in the dataFolderPath for further processing\n",
    "# file_list = glob.glob(json_pattern)\n",
    "\n",
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list.sort()\n",
    "# for file in file_list:\n",
    "#     print(\"reading input file >>> \" + file + \" ...\")\n",
    "#     data=None\n",
    "#     try:\n",
    "#         # read json data from unformatted json file\n",
    "#         data = pd.read_json(file, lines=True)\n",
    "\n",
    "#         if isinstance(data, str):\n",
    "#             data = data['data'][0]['candles']\n",
    "#         elif isinstance(data,pd.DataFrame):\n",
    "#             data = data['data'][0]['candles']\n",
    "#         else:\n",
    "#             data = data.values[0][0]['candles']\n",
    "\n",
    "#     except ValueError:\n",
    "#         # read data from formatted / json linted json file\n",
    "#         data = pd.read_json(file)\n",
    "\n",
    "#         data = data['data'][0]\n",
    "\n",
    "\n",
    "#     inputRawDataDF = inputRawDataDF.append(data, ignore_index=True)\n",
    "#     print(\"File read - SUCCESS\")\n",
    "\n",
    "# #assign column names\n",
    "# inputRawDataDF.columns = ['date-time', 'open',\n",
    "#                           'high', 'low', 'close', 'quantity', 'dont-know']\n",
    "\n",
    "# rawdataDf  = inputRawDataDF \n",
    "# rawdataDf.index = rawdataDf['date-time']\n",
    "# rawdataDf = rawdataDf.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawdataDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawdataDf=rawdataDf.groupby(inputRawDataDF.index).last()\n",
    "# rawdataDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawdataDf.to_csv(projectRootFolderPath+'/01.crudeoil_mar_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
